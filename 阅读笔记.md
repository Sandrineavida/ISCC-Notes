
# Deep evidential fusion with uncertainty quantification and reliability learning for multimodal medical image segmentation
带有 不确定性量化 和 可靠性学习 的 多模态医学图像分割 的深度证据融合

The paper uses the **Dempster-Shafer theory of evidence** to solve the problem of **multimodal medical image segmentation**, which is crucial for comprehensive diagnostic assessment in medical imaging. Here's how the theory is applied:

1. **Deep Evidential Fusion**: The method first extracts features from different imaging modalities (such as MRI or PET/CT) using a deep neural network. These features are then converted into **mass functions** based on Dempster-Shafer theory, where the mass functions represent the amount of evidence that supports each class label (e.g., tumor or non-tumor).

2. **Contextual Discounting**: The mass functions are adjusted by a process called **contextual discounting**, which involves learning **reliability coefficients** for each modality. These coefficients quantify how trustworthy the information from each modality is for each specific class. For example, PET might be more reliable for identifying certain active tumors, while CT might be better for anatomical details.

3. **Combining Evidence**: After adjusting the mass functions, the evidence from each modality is combined using **Dempster’s rule of combination**, which merges the information to create a unified decision regarding each voxel in the image. This fusion of evidence improves the segmentation accuracy and provides a more reliable diagnosis.

4. **Uncertainty Quantification**: The framework also allows for **uncertainty quantification**, helping to identify how confident the system is in its segmentation predictions. This is particularly important in medical imaging, where uncertainty can guide further clinical actions.

This framework was tested on PET-CT and MRI datasets for **tumor segmentation**, showing improvements in both **segmentation accuracy** and **uncertainty quantification** compared to traditional methods【5†source】.

这篇文章利用了**Dempster-Shafer证据理论**来解决**多模态医学图像分割**的问题，具体步骤如下：

1. **深度证据融合**：首先使用深度神经网络从不同的成像模态（例如MRI或PET/CT）中提取特征。这些特征被转换为基于Dempster-Shafer理论的**质量函数**，这些函数表示每个类别标签（例如，肿瘤或非肿瘤）所支持的证据量。

2. **上下文折扣**：质量函数通过**上下文折扣**进行调整，学习到的**可靠性系数**会为每个模态和每个类别量化其信息的可信度。例如，PET可能在识别活跃肿瘤方面更为可靠，而CT在提供解剖细节方面更为准确。

3. **证据融合**：调整后的质量函数会通过**Dempster组合规则**进行融合，从而整合来自不同模态的信息，生成关于图像中每个体素的统一决策。通过这种证据融合，提高了分割的准确性，提供了更可靠的诊断结果。

4. **不确定性量化**：该框架还支持**不确定性量化**，帮助识别系统在分割预测中的置信度。这对于医学影像尤为重要，因为不确定性可以引导进一步的临床决策。

这套框架在PET-CT和MRI数据集上的**肿瘤分割**测试中，展示了相比传统方法的改进，不仅在**分割准确性**上有所提高，还增强了**不确定性量化**的能力【5†source】。

---

# Abstract

单模态医学图像通常无法提供足够的信息来做出准确且可靠的诊断。因此，医生通常依赖多模态医学图像进行全面的诊断评估。本研究引入了一种用于多模态医学图像分割的深度证据融合框架，结合了Dempster-Shafer证据理论和深度神经网络。在该框架中，首先使用深度神经网络从每种成像模态中提取特征，并将这些特征映射到Dempster-Shafer质量函数上，描述每个模态在每个体素上的证据。接着，质量函数通过上下文折扣操作进行修正，使用学习到的系数量化每个信息源相对于各类的可靠性。然后，来自各个模态的折扣证据通过Dempster组合规则进行融合。在PET-CT数据集（用于淋巴瘤分割）和多模态MRI数据集（用于脑肿瘤分割）上进行了实验。结果表明，所提出的融合方案能够量化分割的不确定性并提高分割的准确性。此外，学习到的可靠性系数揭示了每个模态在分割过程中所作贡献的一些见解。

---

# 1. Introduction

近年来，医学成像技术的进步促进了多模态数据的获取，如正电子发射断层扫描（PET）/计算机断层扫描（CT）和多序列磁共振成像（MRI）。单一模态的图像只能提供人体内癌症及其他异常部分的信息，而**多模态医学图像分析**通过整合不同成像模态的信息，显著有助于对复杂医疗状况的全面理解【1】。这包括病理结构的位置、大小和范围等因素。基于多模态医学信息融合的医学图像分割可以帮助临床医生更好地勾画解剖结构、病灶和异常，从而提高疾病检测、诊断和治疗计划的效果。

多模态医学图像融合策略可以在不同的层次上实现【2】。在最低的像素级别上，多模态图像被拼接为一个单一输入；或者，可以从不同的模态中提取特征并结合进行建模和推理（特征级别融合）。最后，在决策级别方法中，会根据每个模态独立地做出部分决策并聚合以得出最终决策。尽管多模态医学图像分析的最新进展取得了有希望的实验结果，传统的多模态医学图像融合策略仍存在一些局限性。通常难以解释为何某个策略在特定环境中有效，并可靠地量化决策中的不确定性。此外，大多数方法基于数据质量的乐观假设，且与临床知识相反，它们在分割肿瘤时将不同模态的图像视为同等可靠，这可能导致偏差或错误的决策。

信息融合的成功依赖于输入信息的相关性和互补性、关于信息源的先验知识的存在以及所采用的不确定性模型的表达能力【3-5】。由于输入信息和先验知识的质量与数据采集阶段紧密相关，许多工作致力于忠实地建模不确定性【6】。作为信息融合过程中的一个关键因素【7，8】，准确的不确定性量化必须作为实现精确的多模态医学图像分割的主要目标。

早期的不确定性量化方法本质上依赖于概率模型，通常结合贝叶斯推理或采样技术来估计各种参数或变量的不确定性【9，10】。深度神经网络的兴起重新激发了人们对不确定性估计的兴趣【7】，从而开发了诸如蒙特卡洛随机失活（Monte-Carlo Dropout）【11】和深度集成【12】等方法。然而，重要的是要注意，这些概率模型依赖于关于底层数据分布的假设，不正确的分布可能导致不准确的不确定性估计。此外，通过推理或采样算法进行的不确定性量化严重依赖于计算近似，且可能缺乏严格的理论依据【13，14】。这些及其他局限性促使人们寻找不确定性量化的替代方法，以用于信息融合和决策应用。

相比于对实际数据分布做出强假设，**非概率方法**使用替代的数学框架或表示形式，如可能性理论【15，16】和Dempster-Shafer理论（DST）【17-19】来量化不确定性。特别是，DST作为一种基于证据的信息建模、推理和融合框架，可用于监督【20-22】和无监督学习【23,24】，为处理不完美（即不精确、不确定和冲突）的数据提供了一种有效方式。与可能性理论相比，DST允许量化**偶然性不确定性**和**认知不确定性**，同时提供了强大的机制来结合多种不可靠的信息【5】。

在多模态医学图像分割中，**有效地结合来自不同来源的不确定信息**是一个重要的挑战。一些基于学习的方法提出通过引入可学习的权重来解决冲突决策【25-27】。在这些方法中，"权重"通常指信息的重要性；而"可靠性"则关系到信息的可信度，需要在不同的医疗情况下进行仔细分析。提供可靠性系数的四种主要方法包括：（1）使用共识度模型化信息源的可靠性【28】；（2）使用概率分布模型化专家意见【29】；（3）使用外部领域知识或上下文信息建模可靠性系数【30】；（4）从训练数据中学习可靠性系数【31，32】，这是一种非常通用的方法，不需要任何领域的先验知识或专家意见。在本文中，我们采用了一种更为灵活的方法，其中每种图像模态的可靠性由多个系数描述，每个系数对应一个真实的类别。对于源 𝑖 和类别 𝑘，可靠性系数定义为当真实类别为 𝑘 时，相信源 𝑖 提供的信息是可靠的程度。

在本文中，我们引入了一种结合DST与深度神经网络的新方法，用于多模态医学图像分割。我们提出的融合方案包括多个基于编码器-解码器的特征提取模块、基于DST的证据映射模块和多模态证据融合模块。证据映射模块将提取的特征转换为质量函数，代表每种成像模态对每个体素类别的证据。然后，这些质量函数通过上下文折扣操作进行修正，并通过Dempster组合规则对修正后的证据进行融合。整个框架通过最小化融合前后每种模态的信息误差的损失函数进行端到端训练。我们主要的贡献如下：

1. 我们提出了一种新的多模态医学图像的混合融合架构，包含特征提取、证据映射和组合模块。
2. 在该架构中，我们整合了以下机制：（i）使用Dempster-Shafer质量函数量化分割的不确定性，（ii）通过上下文折扣修正这些质量函数以考虑每种成像模态的相对可靠性，（iii）结合来自不同来源的修正质量函数以得出最终分割决策。
3. 我们引入了一种改进的双部分损失函数，既可以优化每个单一信息源模态的分割性能，也可以优化组合决策的整体性能。
4. 通过对两个真实医学图像数据集的广泛实验，我们展示了所提出的决策级融合方案相比于像素级方法在可靠性和分割质量方面的提升。
5. 我们展示了学习到的可靠性系数提供了一些关于每种成像模态在分割过程中的贡献的洞察。

本文的其余部分安排如下。第2节回顾了背景信息和相关工作。第3节介绍了我们的方法，第4节报告了实验结果。最后，第5节对本文进行了总结并提出了未来研究的方向。

In this paper, we introduce a new approach to multimodal medical
image segmentation combining DST with deep neural networks.1 The
proposed fusion scheme comprises ***multiple encoder-decoder-based
feature extraction modules***, ***DST-based evidence-mapping modules***, and
a ***multimodality evidence fusion module***. The evidence-mapping modules transform the extracted features into mass functions representing
the evidence from each imaging modality about the class of each voxel.
These mass functions are then corrected by a contextual discounting
operation, and the discounted pieces of evidence are combined by
Dempster’s rule of combination. The whole framework is trained endto-end by minimizing a loss function quantifying the errors before
and after the fusion of information from each modality. Our main
contributions are, thus, the following:

1. We propose a new hybrid fusion architecture for multimodal
medical images composed of feature extraction, evidencemapping, and combination modules.
2. Within this architecture, we integrate mechanisms for (i) quantifying segmentation uncertainty using Dempster–Shafer mass
functions, (ii) correcting these mass functions to account for
the relative reliability of each imaging modality using context
discounting, and (iii) combining corrected mass functions from
different sources to reach final segmentation decisions.
3. We introduce an improved two-part loss function making it
possible to optimize the segmentation performance of each individual source modality together with the overall performance
of the combined decisions.
4. Through extensive experiments with two real medical image
datasets, we show that the proposed decision-level fusion scheme
improves segmentation reliability and quality as compared to
alternative pixel-level methods for exploiting different image
modalities.
5. We show that the learned reliability coefficients provide some
insight into the contribution of each imaging modality in the
segmentation process.
The rest of this paper is organized as follows. Background information and related work are first recalled in Section 2. Our approach is
then introduced in Section 3, and experimental results are reported in
Section 4. Finally, Section 5 concludes the paper and presents some
directions for further research.


> 1 This paper is an extended version of the short paper presented at the
25th International Conference on Medical Image Computing and Computer
Assisted Intervention (MICCAI 2022) [33]. This extended version includes a
much more detailed description and explanation of the **fusion framework**,
an **improved optimization strategy with a two-part loss function**, as well as
extended results with a second dataset for lymphoma segmentation and **an
additional transformer-based feature-extraction module**.

---

# 2. Related work
The basic concepts of DST and its application to classification are
first recalled in Section 2.1. The contextual discounting operation,
which plays a central role in our approach, is described separately in
Section 2.2. The evidential neural network model used in this paper
is then introduced in Section 2.3, and related work on multimodal
medical image fusion is briefly reviewed in Section 2.4.

## 2.1 Dempster–Shafer theory

设 $\Theta = \{\theta_1, \theta_2, \dots, \theta_K\}$ 为某个问题可能答案的有限集合，
称为**识别框架**（frame of discernment）。
关于取值于 $\Theta$ 的变量的证据可以用**质量函数** $m : 2^\Theta \to [0, 1]$ 表示，满足以下条件：

$$
\sum_{A \subseteq \Theta} m(A) = 1 \quad \text{且} \quad m(\emptyset) = 0。
$$

对于每个满足 $m(A) > 0$ 的子集 $A \subseteq \Theta$，
称为质量函数 $m$ 的**焦集**（focal set）。
质量 $m(A)$ 代表分配给焦集 $A$ 的单位信念的份额，而该份额不能分配给 $A$ 的任何严格子集。
(The mass 𝑚(𝐴) represents a share of a unit mass of belief allocated to focal set 𝐴,
which cannot be allocated to any strict subset of 𝐴. )
质量 $m(\Theta)$ 可以解释为无知的程度。完全无知用称为空洞质量函数 $m_?$ 表示，满足 $m_?(\Theta) = 1$。
如果所有焦集都是单元素集，那么 $m$ 被称为贝叶斯型；它等价于概率分布。

### 2.1.1 Belief fct和Plausibility fct
质量函数 $m$ 提供的信息还可以通过**信任函数**（belief function） $Bel$ 
或**可能性函数**（plausibility function） $Pl$ 从 $2^\Theta$ 到 $[0, 1]$ 表示，
定义如下：

$$
Bel(A) = \sum_{B \subseteq A} m(B)
$$

和

<div style="text-align: center;">
  \( Pl(A) = \sum_{B \cap A \neq \emptyset} m(B) = 1 - Bel(\bar{A}), \)
</div>

对于所有 $A \subseteq \Theta$，其中 $\bar{A}$ 表示 $A$ 的补集。
信任函数 $Bel(A)$ 可以解释为对 $A$ 的支持度，而 $Pl(A)$ 是对 $A$ 的反对度的缺失量度。
The quantity
𝐵𝑒𝑙(𝐴) can be interpreted as a degree of support for 𝐴, while 𝑃 𝑙(𝐴) is a
measure of lack of support against 𝐴.


### 2.1.2 Contour fct $pl$
与 $m$ 相关联的**轮廓函数**（contour function） $pl$ 将识别框架
 $\Theta$ 中的每个元素 $\theta$ 映射到其plausibility：

 The contour function 𝑝𝑙 associated to 𝑚 is the function 
 that maps each element 𝜃 of 𝛩 to its plausibility,
i.e.,

$$
pl(\theta) = Pl(\{\theta\}), \quad \forall \theta \in \Theta。
$$

As shown below, this function can be easily computed when combining
several pieces of evidence; it plays an important role in decision-making.

### 2.1.3 Dempster's rule
在 Dempster-Shafer 理论（DST）中，
关于某个问题的信念是通过整合来自相同识别框架的独立证据片段来建立的。
给定由两条独立证据导出的两个质量函数 $m_1$ 和 $m_2$，
表示合并证据的质量函数 $m_1 \oplus m_2$ 定义为：

$$
(m_1 \oplus m_2)(A) = \frac{1}{1 - \kappa} \sum_{B \cap C = A} m_1(B) m_2(C), \tag{1,a}
$$

其中 $\kappa$ 是表示 $m_1$ 和 $m_2$ 之间冲突程度的系数：

$$
\kappa = \sum_{B \cap C = \emptyset} m_1(B) m_2(C)。 \tag{1.b}
$$

这个运算称为**Dempster的组合规则**。它是**交换律**和**结合律**的。
组合的质量函数 $m_1 \oplus m_2$ 称为 $m_1$ 和 $m_2$ 的**正交和**。
只有当 $\kappa < 1$ 时，才能组合质量函数 $m_1$ 和 $m_2$。
令 $pl_1$、$pl_2$ 和 $pl_1 \oplus pl_2$ 分别表示与
 $m_1$、$m_2$ 和 $m_1 \oplus m_2$ 相关的轮廓函数。下式成立：

$$
\forall \theta \in \Theta, \quad (pl_1 \oplus pl_2)(\theta) = \frac{pl_1(\theta)pl_2(\theta)}{1 - \kappa}。 \tag{2}
$$

使用公式（2）计算组合后的轮廓函数的复杂度是关于 $\Theta$ 的基数的线性关系，而使用公式（1）计算组合的质量函数在最坏情况下是指数复杂度。
The complexity of calculating the combined contour function using (2)
is linear in the cardinality of 𝛩, whereas computing the combined mass
function using (1) has, in the worst-case, exponential complexity.

> 【公式（2）中，**轮廓函数** $pl(\theta)$ 直接根据单个元素 $\theta$ 
的可能性计算，这样的计算复杂度与识别框架 $\Theta$ 的基数成线性关系，
因为只需要针对 $\Theta$ 中的每个元素计算 $pl(\theta)$，并将其相乘，
这个操作的复杂度为 $O(|\Theta|)$。

> 而在公式（1）中，**质量函数**的组合涉及的是所有可能的焦集（子集）
$B$ 和 $C$，即需要枚举出识别框架 $\Theta$ 的所有子集。
由于 $\Theta$ 有 $2^{|\Theta|}$ 个子集，因此在最坏的情况下，
计算组合质量函数的复杂度会呈现指数级增长。
因此，公式（1）在最坏情况下的复杂度是**指数复杂度**。】

### 2.1.4 条件化 Conditioning
**翻译：**

**条件化**：给定一个质量函数 $m$ 和识别框架 $\Theta$ 的一个非空子集 $A$，
且满足 $Pl(A) > 0$，条件质量函数 $m(\cdot | A)$ 定义为质量函数 $m$ 
与满足 $m_A(A) = 1$ 的质量函数 $m_A$ 的正交和。

相反，给定一个在 $A$ 上的条件质量函数 $m_0$（表示在只知道真相位于 $A$ 时的信念），
其**条件嵌入**是识别框架 $\Theta$ 上最不精确的质量函数 $m$，使得
 $m(\cdot | A) = m_0$。这个嵌入是通过将每个质量 $m_0(C)$ 转移到 $C \cup A^c$
完成的，对于所有 $C \subseteq A$。
条件嵌入是一种“去条件化”（deconditioning），即它执行条件化的逆运算。

**解释：**

1. **条件化（Conditioning）** 是指在有部分已知信息（
即，真相位于集合 $A$ 中）时，如何更新原有的信念。条件质量函数 $m(\cdot | A)$
 表示在*已知真相位于 $A$ 的情况下*，对识别框架其他子集的信念**重新分配**。

2. **条件嵌入（Conditional Embedding）** 是条件化的逆过程，
称为“去条件化”。它是在已知一个条件质量函数 $m_0$ 的情况下，
找到一个覆盖整个识别框架 $\Theta$ 的质量函数 $m$，
这个 $m$ 在满足条件 $A$ 时等价于 $m_0$。
它通过将 $m_0(C)$ 的质量转移到 $C \cup A^c$ 来完成，
表示增加了一些不确定性，**因为我们在执行条件化时丢失了关于 $A^c$ 的信息**。

简单来说，条件化是根据部分已知信息调整信念，而条件嵌入则是将条件化后的结果扩展回完整的识别框架，恢复一定的模糊性或不确定性。


### 2.1.5 Plausibility-probability transformation
**可能性-概率转换**：一旦表示合并证据的质量函数被计算出来，通常会用来做出决策。DST中的决策方法在文献【35】中进行了回顾。这里，我们将使用最简单的方法【36】，该方法通过归一化单元素集合的可能性来计算识别框架 \(\Theta\) 上的概率分布：

Once a mass function representing **the combined evidence** has been computed, it is often used to make
a decision. Decision-making methods in DST are reviewed in [35].
Here, we will use the simplest method [36], which consists in computing a probability distribution on 𝛩 by normalizing the plausibilities
of the singletons,

$$
p(\theta) = \frac{pl(\theta)}{\sum_{k=1}^K pl(\theta_k)} \quad \forall \theta \in \Theta。\tag{3}
$$

一旦计算出概率，就可以通过最大化期望效用来做出决策。我们注意到，这种方法与Dempster的规则非常契合，因为单元素集合的可能性可以通过公式（2）轻松计算，而无需计算整个组合的质量函数。

Once probabilities have been computed, a decision can be made by
maximizing the expected utility. We note that this method **fits well
with Dempster’s rule (2.1.3)**, as the plausibility of the singletons can be
 easily computed from **(2)** **without computing the whole combined mass
function**.

## 2.2 **建模证据的可靠性**

在DST框架中，可以通过**折扣操作**（discounting operation）来考虑信息源的可靠性，
该操作将质量函数转化为***一个较弱的、信息量较少的质量函数***，
从而使我们能够结合不可靠来源的信息【18】。
设 $m$ 是识别框架 $\Theta$ 上的质量函数，$\beta$ 是 $[0, 1]$ 之间的一个实数，
表示信息源的质量函数 $m$ 的可靠性程度。折扣操作通过折扣率 $1 - \beta$ 将
质量函数 $m$ 转换为一个较少信息的质量函数 $\beta_m$，其定义为 $m$ 和
空洞质量函数 $m_?$ 的加权和，系数为 $\beta$ 和 $1 - \beta$：

$$
\beta_m = \beta m + (1 - \beta) m_?。\tag{4}
$$

> 质量 $m(\Theta)$ 可以解释为无知的程度。完全无知用称为空洞质量函数 $m_?$ 表示，满足 $m_?(\Theta) = 1$。

> Full ignorance is represented
by the vacuous mass function $m_?$ verifying $m_?$(𝛩) = 1.

在本文的其余部分，我们将 $\beta$ 称为**可靠性系数**。当 $\beta = 1$ 时，
我们接受信息源提供的质量函数 $m$，并将其视为我们知识的描述；
当 $\beta = 0$ 时，我们拒绝该信息源，剩下的是空洞质量函数 $m_?$。

**解释**: 折扣操作在DST的许多应用中起着重要作用，
因为它使我们能够考虑关于信息源可靠性的“元知识”。
可以如下解释【37】：
假设质量函数 $m$ 是由一个可能可靠 ($R$) 或不可靠 ($\neg R$) 的信息源提供的。
如果信息源可靠，我们将其意见视为我们的意见，即我们设 $m(\cdot | R) = m$。
如果它不可靠，那么它使我们处于完全无知的状态，
即 $m(\cdot | \neg R) = m_?$。
此外，我们假设我们有以下关于 $R = \{R, \neg R\}$ 的质量函数：
$m_R(\{R\}) = \beta$ 和 $m_R(\neg R) = 1 - \beta$，
即我们认为信息源可靠的程度等于 $\beta$。
Then, combining the conditional embedding of $m(\cdot | R)$ 与 $m_R$，正好得到经过折扣的质量函数 $\beta_m$（如公式(4)所示），在对 $\Theta$ 边缘化后。

> 在 Dempster-Shafer 理论（DST）中，“**marginalizing on $\Theta$**”（在 $\Theta$ 上边缘化）指的是将关于一个较大框架（例如 $\Theta$）的信念或质量函数转化为关于其子集的信念。这种操作通常用于简化信息，或从一个复杂的证据结构中抽取出关于某个子集的信息。

> 具体来说，**边缘化**就是通过聚合某些不感兴趣的变量或忽略掉特定部分来将信念函数（或者质量函数）从复杂的识别框架（$\Theta$）简化为一个较小的框架或子集。例如，假设我们有关于一个较大集合 $\Theta$ 的质量函数 $m$，通过边缘化，我们可以获得关于 $\Theta$ 中某个子集 $A$ 的信念，而忽略与 $A$ 无关的部分。

> 在公式推导中，边缘化通常通过求和的方式进行，即对于每个你关心的部分，去掉（合并）那些不相关的子集。这种操作使得我们能够在更小的框架上做出推断或决策。

> ### 边缘化的一个简单示例：
> 假设 $\Theta = \{A, B, C\}$，我们有一个关于 $\Theta$ 的质量函数 $m$，如果我们只关心关于 $A$ 和 $B$ 的信息（即不关心 $C$），我们可以通过边缘化 $\Theta$ 来忽略 $C$。这样，关于 $A$ 和 $B$ 的新质量函数就是通过将所有涉及 $C$ 的质量分配聚合到 $A$ 和 $B$ 的相关部分上。

> 简而言之，**marginalizing on $\Theta$** 就是将信念或质量函数从 $\Theta$ 中的某些部分“忽略”或“折叠”掉，关注于 $\Theta$ 的某些子集或变量。这通常是为了减少复杂性或只关注感兴趣的部分。


<div style="display: flex; justify-content: space-between;">
  <img src="pairwise_mass_fct_alpha.png" alt="ch2-p26/65" style="width: 33%;">
  <img src="pairwise_mass_fct_alpha_2.png" alt="ch2-p27/65" style="width: 33%;">
  <img src="pairwise_mass_fct_alpha_3.png" alt="ch2-p28/65" style="width: 33%;">
</div>

> $h_{ij}$ 和 $\bar{h}_{ij}$ 是当传感器可信($R$)时候可以获得的元素

> $\Theta_{ij}$: 当传感器不可信($\neg R$)时候,就相当于我们无法获知任何信息，是totally ignorant的，这时候映射到$\Theta_{ij}$

> 在这个图中，$\alpha$ 和 $\alpha'$ 是介于 $[0, 1]$ 之间的系数，表示对传感器信息的**置信度**（degree of confidence）。它衡量我们对传感器测量结果的信任程度。

> 具体地说：
> - 当 $\alpha = 1$ 时，表示我们完全信任传感器的测量结果。
> - 当 $\alpha = 0$ 时，表示我们完全不信任传感器的信息。

> 在这个模型中，$\alpha$ 控制了传感器测量结果在质量函数中的影响程度。较高的 $\alpha$ 值表示我们对传感器的置信度较高，因此依据传感器测量的质量函数会被赋予更多的权重；而较低的 $\alpha$ 则意味着我们对传感器测量结果的信任度较低，因而赋予的不确定性会增加。

> 因此，$\alpha$ 的作用是在结合传感器提供的置信信息和不确定性之间进行权衡。

<div style="display: flex; justify-content: space-between;">
  <img src="pairwise_mass_fct_alpha_4.png" alt="ch2-p29/65" style="width: 49%;">
  <img src="pairwise_mass_fct_alpha_5.png" alt="ch2-p30/66" style="width: 49%;">
</div>


